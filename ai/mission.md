# mission.md
# Boilerplate for Human–AI Analytic Projects
# Version 1.1

## CHAPTER 1: PURPOSE

This file defines the foundational logic, constraints, and epistemological commitments of the analytic project. It outlines the input axioms, inference rules, and interpretation principles by which knowledge claims arising from human–AI collaboration shall be constructed and evaluated.

In a human–AI creative symbiosis, the human serves not merely as an operator, but as a **philosopher–scientist**—the conductor of meaning. Their role is to define the framework within which the AI can execute and translate, but not originate, analytic purpose.

### Epistemic Aim

This project seeks to:  
`[e.g., explore causal relationships, forecast trends, classify latent structures, synthesize multiple data sources, generate hypotheses]`

### Analytic Dialects to Be Used

- Tabular
- Algebraic (mathematical/statistical models)
- Graphical (visualizations)
- Schematic (causal diagrams, network flows)
- Syntactic (code & data logic)
- Numeric (confidence, uncertainty)
- Semantic (human-readable narratives)

---

## CHAPTER 2: ONTOLOGICAL AXIOMS

Foundational truths accepted as self-evident for the purposes of this project. These must be logically consistent and non-negotiable.

### Example Axioms:

- A1. Time is discrete and unidirectional.
- A2. Each observation is an atomic unit of the analytic frame.
- A3. Missing values represent uncertainty, not zero.
- A4. All transformations must be deterministic and reproducible.
- A5. Aggregation across groups must preserve unit definitions.

> Every project must declare its axioms explicitly.

---

## CHAPTER 3: EPISTEMOLOGICAL FRAMEWORK

### Modal Translation Protocols

Claims may be expressed in one modality (e.g. statistical model) but must be **translatable into at least one other** (e.g. visualization, narrative). The human analyst may invoke translation explicitly, or the AI agent may suggest translation points based on complexity or opacity.

### Permitted Reasoning Methods

- Descriptive statistics & visual summaries
- Parametric modeling (e.g., GLMs)
- Simulation-based inference
- Bayesian estimation
- Causal inference (e.g., DAGs, potential outcomes)
- Semantic reasoning from domain-specific corpora

Unpermitted: Any black-box modeling without traceable input-to-output logic.

### Trustworthiness Criteria

- Reproducibility: All outputs must be regenerable.
- Transparency: Each step must be auditable in code or process.
- Cross-modality validation: Claims must align when reexpressed.
- Falsifiability: Claims must permit counter-evidence.

---

## CHAPTER 4: ROLES & RESPONSIBILITIES

**Human Analyst**
- Defines axioms, dialects, interpretation schema
- Issues prompts and re-frames questions
- Interprets outputs with regard to context, ethics, purpose
- Validates or revises AI-generated claims

**AI Agent**
- Translates between modalities
- Executes modeling, summarization, and transformation tasks
- Surfaces uncertainties or anomalies for review
- Adheres to declared reasoning boundaries and trust criteria

### Feedback Loop

At regular intervals or defined milestones:
- Human may request epistemic justification (“why should I trust this?”)
- AI must trace inference chain and translate across modalities
- Conflicting outputs must be flagged for adjudication

---

## CHAPTER 5: OUTPUT VALIDATION

All outputs must pass through these checkpoints:

1. **Internal coherence** (within modality)
2. **Cross-modality translation** (e.g., numeric ↔ semantic ↔ graphical)
3. **Alignment with axioms and declared goals**
4. **Interpretability and usefulness to intended audience**
5. **Optional**: External peer review or contextual validation

---

